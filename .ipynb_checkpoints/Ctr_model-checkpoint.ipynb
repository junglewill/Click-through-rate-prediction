{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.fromnumeric import _all_dispatcher\n",
    "import pandas as pd\n",
    "import joblib\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "soviet-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train.gz...\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "######### training code (without any validation) #########\n",
    "\n",
    "# load data\n",
    "print('loading train.gz...')\n",
    "# use only a subset of rows - you should use all rows eventually\n",
    "df_train = pd.read_csv(\"train.gz\", compression='gzip', nrows=20000, header='infer')\n",
    "Y = df_train['click']\n",
    "# discard some columns\n",
    "unused_cols = [\"id\", 'site_id', 'app_id']\n",
    "df_train.drop(unused_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "catholic-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy to prevent from modifying the original dataset\n",
    "df_copy = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "specialized-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique() shows that the three columns are same across \n",
    "df_copy.drop(['hour', 'app_domain', 'app_category', 'click'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "reverse-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_copy, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "swedish-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14000 entries, 12839 to 18558\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   C1                14000 non-null  int64 \n",
      " 1   banner_pos        14000 non-null  int64 \n",
      " 2   site_domain       14000 non-null  object\n",
      " 3   site_category     14000 non-null  object\n",
      " 4   device_id         14000 non-null  object\n",
      " 5   device_ip         14000 non-null  object\n",
      " 6   device_model      14000 non-null  object\n",
      " 7   device_type       14000 non-null  int64 \n",
      " 8   device_conn_type  14000 non-null  int64 \n",
      " 9   C14               14000 non-null  int64 \n",
      " 10  C15               14000 non-null  int64 \n",
      " 11  C16               14000 non-null  int64 \n",
      " 12  C17               14000 non-null  int64 \n",
      " 13  C18               14000 non-null  int64 \n",
      " 14  C19               14000 non-null  int64 \n",
      " 15  C20               14000 non-null  int64 \n",
      " 16  C21               14000 non-null  int64 \n",
      "dtypes: int64(12), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "passing-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# one_hot_features = ['C1', 'device_type', 'device_conn_type', 'C18']\n",
    "one_hot_features = [0,7,8,13]\n",
    "one_hot_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# target_features = ['banner_pos','site_domain','site_category','device_id','device_ip','device_model','C14',\n",
    "#                    'C15','C16','C17','C19','C20','C21']\n",
    "target_features = [1,2,3,4,5,6,9,10,11,12,14,15,16]\n",
    "target_transformer = ce.JamesSteinEncoder()\n",
    "        \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot', one_hot_transformer, one_hot_features),\n",
    "        ('target', target_transformer, target_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "monthly-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2405698\ttotal: 410ms\tremaining: 410ms\n",
      "1:\tlearn: 0.1699838\ttotal: 765ms\tremaining: 0us\n",
      "model logloss: 0.530\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', CatBoostClassifier(iterations=2,learning_rate=1,depth=15))])\n",
    "\n",
    "clf.fit(X_train.values, y_train.values.reshape(-1,1))\n",
    "y_pred = clf.predict_proba(X_test.values)[:, 1]\n",
    "print(\"model logloss: %.3f\" % log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-gossip",
   "metadata": {},
   "source": [
    "#### First baseline model has a very high logloss at 0.525."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "textile-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>banner_pos</th>\n",
       "      <th>site_domain</th>\n",
       "      <th>site_category</th>\n",
       "      <th>device_id</th>\n",
       "      <th>device_ip</th>\n",
       "      <th>device_model</th>\n",
       "      <th>device_type</th>\n",
       "      <th>device_conn_type</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>ac78510a</td>\n",
       "      <td>f66779e6</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>81a9de12</td>\n",
       "      <td>3223bcfe</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20362</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1c7daf87</td>\n",
       "      <td>f66779e6</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>febeb42a</td>\n",
       "      <td>a0f5f879</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17654</td>\n",
       "      <td>181</td>\n",
       "      <td>231</td>\n",
       "      <td>1572</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17699</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3eda54e5</td>\n",
       "      <td>74073276</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>a4459495</td>\n",
       "      <td>1f0bc64f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17614</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1571</td>\n",
       "      <td>2</td>\n",
       "      <td>1031</td>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dd641cc7</td>\n",
       "      <td>8fd0aea4</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>a776d5c6</td>\n",
       "      <td>711ee120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15705</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1300</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15374</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3eda54e5</td>\n",
       "      <td>74073276</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>c09833cf</td>\n",
       "      <td>5096d134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17753</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1571</td>\n",
       "      <td>2</td>\n",
       "      <td>1031</td>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18781</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>ac78510a</td>\n",
       "      <td>f66779e6</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>4b6fe0a2</td>\n",
       "      <td>d787e91b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20352</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16347</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1c7daf87</td>\n",
       "      <td>f66779e6</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>fdc6efc3</td>\n",
       "      <td>4ea23a13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17653</td>\n",
       "      <td>181</td>\n",
       "      <td>231</td>\n",
       "      <td>1572</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>832f52bc</td>\n",
       "      <td>74073276</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>b8545261</td>\n",
       "      <td>698a4073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16858</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1465</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17536</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dd641cc7</td>\n",
       "      <td>8fd0aea4</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>61c36f0b</td>\n",
       "      <td>81b42528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15703</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1300</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6201</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>d74367ae</td>\n",
       "      <td>74073276</td>\n",
       "      <td>a99f214a</td>\n",
       "      <td>d3217aa6</td>\n",
       "      <td>be6db1d7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19950</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>1378</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>76</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       C1  banner_pos site_domain site_category device_id device_ip  \\\n",
       "10657   5           0    ac78510a      f66779e6  a99f214a  81a9de12   \n",
       "3375    5           0    1c7daf87      f66779e6  a99f214a  febeb42a   \n",
       "17699   5           1    3eda54e5      74073276  a99f214a  a4459495   \n",
       "10605   5           0    dd641cc7      8fd0aea4  a99f214a  a776d5c6   \n",
       "15374   5           1    3eda54e5      74073276  a99f214a  c09833cf   \n",
       "...    ..         ...         ...           ...       ...       ...   \n",
       "18781   5           0    ac78510a      f66779e6  a99f214a  4b6fe0a2   \n",
       "16347   5           0    1c7daf87      f66779e6  a99f214a  fdc6efc3   \n",
       "2669    5           0    832f52bc      74073276  a99f214a  b8545261   \n",
       "17536   5           0    dd641cc7      8fd0aea4  a99f214a  61c36f0b   \n",
       "6201    5           0    d74367ae      74073276  a99f214a  d3217aa6   \n",
       "\n",
       "      device_model  device_type  device_conn_type    C14  C15  C16   C17  C18  \\\n",
       "10657     3223bcfe            1                 0  20362  201   31  1911    0   \n",
       "3375      a0f5f879            1                 0  17654  181  231  1572    2   \n",
       "17699     1f0bc64f            1                 0  17614  201   31  1571    2   \n",
       "10605     711ee120            1                 0  15705  201   31  1300    0   \n",
       "15374     5096d134            1                 0  17753  201   31  1571    2   \n",
       "...            ...          ...               ...    ...  ...  ...   ...  ...   \n",
       "18781     d787e91b            1                 0  20352  201   31  1911    0   \n",
       "16347     4ea23a13            1                 0  17653  181  231  1572    2   \n",
       "2669      698a4073            1                 0  16858  201   31  1465    3   \n",
       "17536     81b42528            1                 0  15703  201   31  1300    0   \n",
       "6201      be6db1d7            1                 0  19950  201   31  1378    3   \n",
       "\n",
       "        C19  C20  C21  \n",
       "10657     7   -1  145  \n",
       "3375      7   -1   21  \n",
       "17699  1031   84   21  \n",
       "10605     3   85   67  \n",
       "15374  1031   84   21  \n",
       "...     ...  ...  ...  \n",
       "18781     7   -1  145  \n",
       "16347     7   -1   21  \n",
       "2669      7   -1   11  \n",
       "17536     3   -1   67  \n",
       "6201    135   76   11  \n",
       "\n",
       "[14000 rows x 17 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ce.JamesSteinEncoder(cols=['site_domain'])\n",
    "encoder.fit_transform(X_train, y_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "uniform-induction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 5]), array([  1,  97, 181, 201]), array([  1,  17,  31, 231])]\n"
     ]
    }
   ],
   "source": [
    "on_hot = OneHotEncoder(drop='first')\n",
    "on_hot.fit(X_train[['C1', 'C15', 'C16']])\n",
    "print(on_hot.categories_)\n",
    "A = on_hot.transform(X_train[['C1', 'C15', 'C16']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "significant-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "tight-orleans",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14000 entries, 10657 to 6201\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   C1                14000 non-null  int64 \n",
      " 1   banner_pos        14000 non-null  int64 \n",
      " 2   site_domain       14000 non-null  object\n",
      " 3   site_category     14000 non-null  object\n",
      " 4   device_id         14000 non-null  object\n",
      " 5   device_ip         14000 non-null  object\n",
      " 6   device_model      14000 non-null  object\n",
      " 7   device_type       14000 non-null  int64 \n",
      " 8   device_conn_type  14000 non-null  int64 \n",
      " 9   C14               14000 non-null  int64 \n",
      " 10  C15               14000 non-null  int64 \n",
      " 11  C16               14000 non-null  int64 \n",
      " 12  C17               14000 non-null  int64 \n",
      " 13  C18               14000 non-null  int64 \n",
      " 14  C19               14000 non-null  int64 \n",
      " 15  C20               14000 non-null  int64 \n",
      " 16  C21               14000 non-null  int64 \n",
      "dtypes: int64(12), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# need to balance the data\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization. \n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(random_params, i)\n",
    "        results.loc[i, :] = eval_results\n",
    "\n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "       'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "       'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "       'degree': Integer(1,8),\n",
    "       'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "    },\n",
    "    n_iter=32,\n",
    "   random_state=0\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "= opt.fit(X_train, y_train)\n",
    "\n",
    "# model can be saved, used for predictions or scoring\n",
    "print(opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-insertion",
   "metadata": {},
   "source": [
    "### Logistic Regression (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "capital-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding...\n",
      "imported data & built a vectorizer on the training set\n",
      "n = 20000, d = 14076\n"
     ]
    }
   ],
   "source": [
    "# embedding (all features are categorical)\n",
    "print('embedding...')\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('X_train_dict.pkl', 'rb') as ff:\n",
    "        X_train_dict = pickle.load(ff)\n",
    "    vectorizer = joblib.load('vectorizer.joblib')\n",
    "    X_train = vectorizer.transform(X_train_dict)\n",
    "    print('saved vectorizer loaded & applied to training set')\n",
    "except:\n",
    "    X_train_dict = list(df_copy.drop('click', axis=1).T.to_dict().values())\n",
    "    with open('X_train_dict.pkl', 'wb') as ff:\n",
    "        pickle.dump(X_train_dict, ff)\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X_train = vectorizer.fit_transform(X_train_dict) # can only see training dataset\n",
    "    joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "    print('imported data & built a vectorizer on the training set')\n",
    "\n",
    "n, d = X_train.shape\n",
    "print(\"n = {}, d = {}\".format(n, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlike-collective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x14076 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 340000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "horizontal-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit a simple logistic regression with l1 regularization...\n",
      "...done training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "print('fit a simple logistic regression with l1 regularization...')\n",
    "clf = LogisticRegression(max_iter=20000, penalty='l1', solver='liblinear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print('...done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "challenging-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and transforming test data...\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "######### testing code ###################################\n",
    "\n",
    "# transform test data as well\n",
    "print('loading and transforming test data...')\n",
    "df_test = pd.read_csv(\"test.gz\", compression='gzip', header='infer')\n",
    "# df_test.set_index('id', inplace=True)\n",
    "unused_cols = ['site_id', 'app_id']\n",
    "df_test.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "try:\n",
    "    with open('X_test_dict.pkl', 'rb') as ff:\n",
    "        X_test_dict = pickle.load('ff')\n",
    "except:\n",
    "    X_test_dict = list(df_test.T.to_dict().values())\n",
    "    with open('X_test_dict.pkl', 'wb') as ff:\n",
    "        pickle.dump(X_test_dict, ff)\n",
    "\n",
    "X_test = vectorizer.transform(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "technical-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting and output to csv...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "print('predicting and output to csv...')\n",
    "ctr_pred = clf.predict_proba(X_test)[:, 1]\n",
    "# save output: every line is (id, ctr_pred)\n",
    "all_id = df_test['id']\n",
    "df_out = pd.DataFrame({'id': all_id, 'ctr': ctr_pred})\n",
    "df_out.to_csv('Submission.csv', index=False)\n",
    "\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
