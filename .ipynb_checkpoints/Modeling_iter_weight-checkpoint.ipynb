{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incredible-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_df.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "renewable-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_f = ['hour', 'C1', 'banner_pos', 'site_id', 'site_domain','site_category','device_id','device_ip','device_model',\n",
    "                 'device_type','device_conn_type', 'C14','C15','C16','C17','C18', 'C19','C20','C21',\n",
    "                 'day_of_week', 'user', 'click_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crucial-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_obj_to_int(self):\n",
    "    \n",
    "    object_list_columns = self.columns\n",
    "    object_list_dtypes = self.dtypes\n",
    "    new_col_suffix = '_int'\n",
    "    for index in range(0,len(object_list_columns)):\n",
    "        if object_list_dtypes[index] == object :\n",
    "            self[object_list_columns[index]+new_col_suffix] = self[object_list_columns[index]].map( lambda  x: hash(x))\n",
    "            self.drop([object_list_columns[index]],inplace=True,axis=1)\n",
    "    return self\n",
    "\n",
    "X_train = df.drop([\"click\"],axis=1)\n",
    "y_train = df[\"click\"]\n",
    "X_train_hash = X_train.copy()\n",
    "column_list = ['C1', 'banner_pos', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', \n",
    "               'C19', 'C20', 'C21']\n",
    "X_train_hash[column_list] = X_train_hash[column_list].astype('object')\n",
    "X_train_hash = convert_obj_to_int(X_train_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incredible-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bootstrapped = df.sample(n=len(df), replace=True, random_state=101)\n",
    "# X_boot = df_bootstrapped.drop([\"click\"],axis=1)\n",
    "# y_boot = df_bootstrapped[\"click\"]\n",
    "# X_boot_hash = X_boot.copy()\n",
    "# X_boot_hash[column_list] = X_boot_hash[column_list].astype('object')\n",
    "# X_boot_hash = convert_obj_to_int(X_boot_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bootstrapped_2 = df.sample(n=len(df), replace=True, random_state=36)\n",
    "# X_boot_2 = df_bootstrapped_2.drop([\"click\"],axis=1)\n",
    "# y_boot_2 = df_bootstrapped_2[\"click\"]\n",
    "# X_boot_hash_2 = X_boot_2.copy()\n",
    "# X_boot_hash_2[column_list] = X_boot_hash_2[column_list].astype('object')\n",
    "# X_boot_hash_2 = convert_obj_to_int(X_boot_hash_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "experimental-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bootstrapped_3 = df.sample(n=len(df), replace=True, random_state=70)\n",
    "# X_boot_3 = df_bootstrapped_3.drop([\"click\"],axis=1)\n",
    "# y_boot_3 = df_bootstrapped_3[\"click\"]\n",
    "# X_boot_hash_3 = X_boot_3.copy()\n",
    "# X_boot_hash_3[column_list] = X_boot_hash_3[column_list].astype('object')\n",
    "# X_boot_hash_3 = convert_obj_to_int(X_boot_hash_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pharmaceutical-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"sj_test.gz\", compression=\"gzip\")\n",
    "X_test = test.drop([\"click\"],axis=1)\n",
    "y_test = test[\"click\"]\n",
    "X_test_hash = X_test.copy()\n",
    "column_list = ['C1', 'banner_pos', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', \n",
    "               'C19', 'C20', 'C21']\n",
    "X_test_hash[column_list] = X_test_hash[column_list].astype('object')\n",
    "X_test_hash = convert_obj_to_int(X_test_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "concerned-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal hyperparameters\n",
    "best_p_l = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 5, \n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.11304216699488043,\n",
    "    'feature_fraction': 0.5066204305086464,\n",
    "    'bagging_fraction': 0.6657456066570288,\n",
    "    'max_bin': 188,\n",
    "    'n_estimators': 482,\n",
    "    'num_leaves': 60,\n",
    "    'min_sum_hessian_in_leaf':72\n",
    "}\n",
    "\n",
    "best_p_l_8 = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 2, \n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.037681961372348104,\n",
    "    'feature_fraction': 0.6186329542584896,\n",
    "    'bagging_fraction': 0.7686771918501543,\n",
    "    'max_bin': 198,\n",
    "    'n_estimators': 854,\n",
    "    'num_leaves': 58,\n",
    "    'min_sum_hessian_in_leaf':34,\n",
    "    'lambda_l1': 8.027647813535458,\n",
    "    'lambda_l2': 5.230523285313312,\n",
    "    'min_data_in_leaf': 93,\n",
    "    'min_split_gain': 0.03929273115755069\n",
    "}\n",
    "\n",
    "best_p_l_28 = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 7, \n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.3,\n",
    "    'feature_fraction': 0.8999999999999999,\n",
    "    'bagging_fraction': 0.8999999999999999,\n",
    "    'max_bin': 59,\n",
    "    'n_estimators': 218,\n",
    "    'num_leaves': 80,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'lambda_l1': 1e-08,\n",
    "    'lambda_l2': 3.3855221440653636,\n",
    "    'min_data_in_leaf':26,\n",
    "    'min_split_gain': 0.03132393135883699\n",
    "}\n",
    "\n",
    "best_p_l_4812 = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 1, \n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.17830921881011944,\n",
    "    'feature_fraction': 0.5866476844693964,\n",
    "    'bagging_fraction': 0.8331767682764499,\n",
    "    'max_bin': 255,\n",
    "    'n_estimators': 283,\n",
    "    'num_leaves': 24,\n",
    "    'min_sum_hessian_in_leaf':4,\n",
    "    'lambda_l1': 4.227555053091508,\n",
    "    'lambda_l2': 3.7698218608912613,\n",
    "    'min_data_in_leaf':65,\n",
    "    'min_split_gain': 0.024848679158260303\n",
    "}\n",
    "\n",
    "best_p_l_19 = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 7, \n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.3,\n",
    "    'feature_fraction': 0.8999999999999999,\n",
    "    'bagging_fraction': 0.8999999999999999,\n",
    "    'max_bin': 255,\n",
    "    'n_estimators': 1000,\n",
    "    'num_leaves': 80,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'lambda_l1': 10.0,\n",
    "    'lambda_l2': 1e-08,\n",
    "    'min_data_in_leaf':20,\n",
    "    'min_split_gain': 0.1\n",
    "}\n",
    "\n",
    "best_p_l_48128 = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric':'binary_logloss', \n",
    "    'bagging_freq': 1, \n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.29287582078739416,\n",
    "    'feature_fraction': 0.8999999999999999,\n",
    "    'bagging_fraction': 0.8999999999999999,\n",
    "    'max_bin': 255,\n",
    "    'n_estimators': 625,\n",
    "    'num_leaves': 80,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'lambda_l1': 10.0,\n",
    "    'lambda_l2': 1e-08,\n",
    "    'min_data_in_leaf':20,\n",
    "    'min_split_gain': 0.001\n",
    "}\n",
    "\n",
    "best_p_x = {\n",
    "    'alpha':0.0,\n",
    "    'colsample_bytree':1.0,\n",
    "    'gamma':0.0,\n",
    "    'iterations':400,\n",
    "    'learning_rate':0.27950642975302614,\n",
    "    'max_depth':5,\n",
    "    'n_estimators':100,\n",
    "    'subsample':1.0\n",
    "}\n",
    "\n",
    "best_p_c_2 = {\n",
    "    'colsample_bylevel':1.0,\n",
    "    'iterations': 250,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 1000,\n",
    "    'leaf_estimation_iterations': 5,\n",
    "    'model_size_reg': 0.001,\n",
    "    'random_strength': 10.0,\n",
    "    'scale_pos_weight':1.0,\n",
    "    'subsample':1.0\n",
    "}\n",
    "\n",
    "best_p_c_240 = {\n",
    "    'colsample_bylevel':0.6,\n",
    "    'iterations': 250,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 1000,\n",
    "    'leaf_estimation_iterations': 1,\n",
    "    'model_size_reg': 0.001,\n",
    "    'random_strength': 1e-09,\n",
    "    'scale_pos_weight':1.0,\n",
    "    'subsample':0.6\n",
    "}\n",
    "\n",
    "best_p_c_101 = {\n",
    "    'colsample_bylevel':0.5,\n",
    "    'iterations': 250,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 0.001,\n",
    "    'leaf_estimation_iterations': 1,\n",
    "    'model_size_reg': 0.01,\n",
    "    'random_strength': 10,\n",
    "    'scale_pos_weight':1.0,\n",
    "    'subsample':1.0\n",
    "}\n",
    "\n",
    "best_p_c_16 = {\n",
    "    'colsample_bylevel':1.0,\n",
    "    'iterations': 250,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 10000.0,\n",
    "    'leaf_estimation_iterations': 1,\n",
    "    'model_size_reg': 0.01,\n",
    "    'random_strength': 1e-9,\n",
    "    'scale_pos_weight':1.0,\n",
    "    'subsample':1.0\n",
    "}\n",
    "\n",
    "best_p_c_24 = {\n",
    "    'colsample_bylevel':1.0,\n",
    "    'iterations': 250,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 0.001,\n",
    "    'leaf_estimation_iterations': 5,\n",
    "    'model_size_reg': 0.01,\n",
    "    'random_strength': 1e-9,\n",
    "    'scale_pos_weight':1.0,\n",
    "    'subsample':1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-caution",
   "metadata": {},
   "source": [
    "### train the optimal models on the train_df, to obtain the optimal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "analyzed-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: loss_function\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6657456066570288, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6657456066570288\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5066204305086464, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5066204305086464\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=72, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=72\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    }
   ],
   "source": [
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True) \n",
    "logloss_l=[]\n",
    "proba_lgb_li=[]\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(**best_p_l, loss_function='Logloss')\n",
    "lgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_lgb_li.append(proba_lgb)\n",
    "logloss_l.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash,y_boot,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li.append(proba_lgb)\n",
    "# logloss_l.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_2,y_boot_2,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li.append(proba_lgb)\n",
    "# logloss_l.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_3,y_boot_3,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li.append(proba_lgb)\n",
    "# logloss_l.append(log_loss(y_test, proba_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "preliminary-blanket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4549539661539116]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: loss_function\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6186329542584896, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6186329542584896\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.027647813535458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.027647813535458\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7686771918501543, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7686771918501543\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.230523285313312, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.230523285313312\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=34, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=34\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    }
   ],
   "source": [
    "logloss_l_8=[]\n",
    "proba_lgb_li_8=[]\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(**best_p_l_8, loss_function='Logloss')\n",
    "lgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_lgb_li_8.append(proba_lgb)\n",
    "logloss_l_8.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_8, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash,y_boot,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_8.append(proba_lgb)\n",
    "# logloss_l_8.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_8, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_2,y_boot_2,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_8.append(proba_lgb)\n",
    "# logloss_l_8.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_8, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_3,y_boot_3,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_8.append(proba_lgb)\n",
    "# logloss_l_8.append(log_loss(y_test, proba_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stupid-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: loss_function\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8999999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8999999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=1e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999999999999999, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999999999999999\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.3855221440653636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3855221440653636\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    }
   ],
   "source": [
    "logloss_l_28=[]\n",
    "proba_lgb_li_28=[]\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(**best_p_l_28, loss_function='Logloss')\n",
    "lgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_lgb_li_28.append(proba_lgb)\n",
    "logloss_l_28.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_28, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash,y_boot,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_28.append(proba_lgb)\n",
    "# logloss_l_28.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_28, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_2,y_boot_2,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_28.append(proba_lgb)\n",
    "# logloss_l_28.append(log_loss(y_test, proba_lgb))\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# lgb = lgb.LGBMClassifier(**best_p_l_28, loss_function='Logloss')\n",
    "# lgb.fit(X_boot_hash_3,y_boot_3,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_lgb_li_28.append(proba_lgb)\n",
    "# logloss_l_28.append(log_loss(y_test, proba_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: loss_function\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5866476844693964, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5866476844693964\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.227555053091508, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.227555053091508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8331767682764499, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8331767682764499\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.7698218608912613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.7698218608912613\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=4, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    }
   ],
   "source": [
    "logloss_l_4812=[]\n",
    "proba_lgb_li_4812=[]\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(**best_p_l_4812, loss_function='Logloss')\n",
    "lgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_lgb_li_4812.append(proba_lgb)\n",
    "logloss_l_4812.append(log_loss(y_test, proba_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "champion-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: loss_function\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8999999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8999999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=10.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999999999999999, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999999999999999\n",
      "[LightGBM] [Warning] lambda_l2 is set=1e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1e-08\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    }
   ],
   "source": [
    "logloss_l_19=[]\n",
    "proba_lgb_li_19=[]\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(**best_p_l_19, loss_function='Logloss')\n",
    "lgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_lgb = lgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_lgb_li_19.append(proba_lgb)\n",
    "logloss_l_19.append(log_loss(y_test, proba_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "frequent-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4553069584127081]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_l_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mobile-norman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:18:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { iterations, loss_function } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:18:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "logloss_x=[]\n",
    "proba_xgb_li=[]\n",
    "\n",
    "xgb = XGBClassifier(**best_p_x, loss_function='Logloss')\n",
    "xgb.fit(X_train_hash,y_train,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_xgb = xgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "proba_xgb_li.append(proba_xgb)\n",
    "logloss_x.append(log_loss(y_test, proba_xgb))\n",
    "\n",
    "# xgb = XGBClassifier(**best_p_x, loss_function='Logloss')\n",
    "# xgb.fit(X_boot_hash_2,y_boot_2,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_xgb = xgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_xgb_li.append(proba_xgb)\n",
    "# logloss_x.append(log_loss(y_test, proba_xgb))\n",
    "\n",
    "# xgb = XGBClassifier(**best_p_x, loss_function='Logloss')\n",
    "# xgb.fit(X_boot_hash_3,y_boot_3,eval_set=[(X_test_hash,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "# proba_xgb = xgb.predict_proba(X_test_hash.values)[:, 1]\n",
    "# proba_xgb_li.append(proba_xgb)\n",
    "# logloss_x.append(log_loss(y_test, proba_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "breathing-bargain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4590530070074255]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "treated-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_c_2=[]\n",
    "proba_cat_li_2=[]\n",
    "\n",
    "cat = CatBoostClassifier(**best_p_c_2,od_type='Iter', loss_function='Logloss', cat_features=categorical_f)\n",
    "cat.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "proba_cat_li_2.append(proba_cat)\n",
    "logloss_c_2.append(log_loss(y_test, proba_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hazardous-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_c_240=[]\n",
    "proba_cat_li_240=[]\n",
    "\n",
    "cat = CatBoostClassifier(**best_p_c_240,od_type='Iter', loss_function='Logloss', cat_features=categorical_f)\n",
    "cat.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "proba_cat_li_240.append(proba_cat)\n",
    "logloss_c_240.append(log_loss(y_test, proba_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "failing-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_c_101=[]\n",
    "proba_cat_li_101=[]\n",
    "\n",
    "cat = CatBoostClassifier(**best_p_c_101,od_type='Iter', loss_function='Logloss', cat_features=categorical_f)\n",
    "cat.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "proba_cat_li_101.append(proba_cat)\n",
    "logloss_c_101.append(log_loss(y_test, proba_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hundred-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_c_16=[]\n",
    "proba_cat_li_16=[]\n",
    "\n",
    "cat = CatBoostClassifier(**best_p_c_16,od_type='Iter', loss_function='Logloss', cat_features=categorical_f)\n",
    "cat.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "proba_cat_li_16.append(proba_cat)\n",
    "logloss_c_16.append(log_loss(y_test, proba_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "automatic-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_c_24=[]\n",
    "proba_cat_li_24=[]\n",
    "\n",
    "cat = CatBoostClassifier(**best_p_c_24,od_type='Iter', loss_function='Logloss', cat_features=categorical_f)\n",
    "cat.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100, verbose=False)\n",
    "proba_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "proba_cat_li_24.append(proba_cat)\n",
    "logloss_c_24.append(log_loss(y_test, proba_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "equivalent-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb48128 = pd.read_csv('lgb48128_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "surgical-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_lgb_li_48128 = np.array(lgb48128['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "prescription-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict = {\n",
    "    'lgbm': proba_lgb_li[0],\n",
    "    'lgbm_8': proba_lgb_li_8[0],\n",
    "    'lgbm_28': proba_lgb_li_28[0],\n",
    "    'lgbm_4812':proba_lgb_li_4812[0],\n",
    "    'lgbm_19': proba_lgb_li_19[0],\n",
    "    'lgbm_48128': proba_lgb_li_48128,\n",
    "    'xgb':proba_xgb_li[0],\n",
    "    'cat_2':proba_cat_li_2[0],\n",
    "    'cat_240': proba_cat_li_240[0],\n",
    "    'cat_101': proba_cat_li_101[0],\n",
    "    'cat_16': proba_cat_li_16[0],\n",
    "    'cat_24': proba_cat_li_24[0],\n",
    "}\n",
    "prob_df = pd.DataFrame(prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-leeds",
   "metadata": {},
   "source": [
    "# Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adaptive-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm</th>\n",
       "      <th>lgbm_8</th>\n",
       "      <th>lgbm_28</th>\n",
       "      <th>lgbm_4812</th>\n",
       "      <th>lgbm_19</th>\n",
       "      <th>lgbm_48128</th>\n",
       "      <th>xgb</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_240</th>\n",
       "      <th>cat_101</th>\n",
       "      <th>cat_16</th>\n",
       "      <th>cat_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720769</td>\n",
       "      <td>0.708838</td>\n",
       "      <td>0.743060</td>\n",
       "      <td>0.666438</td>\n",
       "      <td>0.731289</td>\n",
       "      <td>0.634651</td>\n",
       "      <td>0.742674</td>\n",
       "      <td>0.557389</td>\n",
       "      <td>0.520828</td>\n",
       "      <td>0.510867</td>\n",
       "      <td>0.522435</td>\n",
       "      <td>0.344089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.259261</td>\n",
       "      <td>0.195213</td>\n",
       "      <td>0.246929</td>\n",
       "      <td>0.211735</td>\n",
       "      <td>0.303465</td>\n",
       "      <td>0.135107</td>\n",
       "      <td>0.134947</td>\n",
       "      <td>0.160330</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.155188</td>\n",
       "      <td>0.180702</td>\n",
       "      <td>0.171974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.178526</td>\n",
       "      <td>0.163285</td>\n",
       "      <td>0.161672</td>\n",
       "      <td>0.174675</td>\n",
       "      <td>0.151295</td>\n",
       "      <td>0.128034</td>\n",
       "      <td>0.177632</td>\n",
       "      <td>0.164874</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>0.156001</td>\n",
       "      <td>0.167668</td>\n",
       "      <td>0.150325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258672</td>\n",
       "      <td>0.279451</td>\n",
       "      <td>0.285455</td>\n",
       "      <td>0.258013</td>\n",
       "      <td>0.194119</td>\n",
       "      <td>0.529545</td>\n",
       "      <td>0.269195</td>\n",
       "      <td>0.267091</td>\n",
       "      <td>0.235199</td>\n",
       "      <td>0.255933</td>\n",
       "      <td>0.249617</td>\n",
       "      <td>0.219441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224993</td>\n",
       "      <td>0.241033</td>\n",
       "      <td>0.228025</td>\n",
       "      <td>0.198196</td>\n",
       "      <td>0.246160</td>\n",
       "      <td>0.264212</td>\n",
       "      <td>0.181404</td>\n",
       "      <td>0.292495</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.287250</td>\n",
       "      <td>0.256893</td>\n",
       "      <td>0.255608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632698</th>\n",
       "      <td>0.230936</td>\n",
       "      <td>0.248003</td>\n",
       "      <td>0.257737</td>\n",
       "      <td>0.249031</td>\n",
       "      <td>0.282837</td>\n",
       "      <td>0.226353</td>\n",
       "      <td>0.274747</td>\n",
       "      <td>0.245429</td>\n",
       "      <td>0.240252</td>\n",
       "      <td>0.254258</td>\n",
       "      <td>0.257292</td>\n",
       "      <td>0.218767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632699</th>\n",
       "      <td>0.167198</td>\n",
       "      <td>0.192693</td>\n",
       "      <td>0.133136</td>\n",
       "      <td>0.180714</td>\n",
       "      <td>0.142688</td>\n",
       "      <td>0.212325</td>\n",
       "      <td>0.138745</td>\n",
       "      <td>0.168712</td>\n",
       "      <td>0.177035</td>\n",
       "      <td>0.164318</td>\n",
       "      <td>0.214233</td>\n",
       "      <td>0.141854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632700</th>\n",
       "      <td>0.154964</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.151572</td>\n",
       "      <td>0.169375</td>\n",
       "      <td>0.135997</td>\n",
       "      <td>0.167799</td>\n",
       "      <td>0.163699</td>\n",
       "      <td>0.178346</td>\n",
       "      <td>0.162533</td>\n",
       "      <td>0.182450</td>\n",
       "      <td>0.158352</td>\n",
       "      <td>0.170247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632701</th>\n",
       "      <td>0.262074</td>\n",
       "      <td>0.276528</td>\n",
       "      <td>0.274755</td>\n",
       "      <td>0.264907</td>\n",
       "      <td>0.285704</td>\n",
       "      <td>0.311325</td>\n",
       "      <td>0.289288</td>\n",
       "      <td>0.301740</td>\n",
       "      <td>0.280562</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.298746</td>\n",
       "      <td>0.277619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632702</th>\n",
       "      <td>0.258330</td>\n",
       "      <td>0.282772</td>\n",
       "      <td>0.274016</td>\n",
       "      <td>0.253963</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.336860</td>\n",
       "      <td>0.286293</td>\n",
       "      <td>0.245553</td>\n",
       "      <td>0.241083</td>\n",
       "      <td>0.258185</td>\n",
       "      <td>0.227552</td>\n",
       "      <td>0.273197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632703 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lgbm    lgbm_8   lgbm_28  lgbm_4812   lgbm_19  lgbm_48128  \\\n",
       "0       0.720769  0.708838  0.743060   0.666438  0.731289    0.634651   \n",
       "1       0.259261  0.195213  0.246929   0.211735  0.303465    0.135107   \n",
       "2       0.178526  0.163285  0.161672   0.174675  0.151295    0.128034   \n",
       "3       0.258672  0.279451  0.285455   0.258013  0.194119    0.529545   \n",
       "4       0.224993  0.241033  0.228025   0.198196  0.246160    0.264212   \n",
       "...          ...       ...       ...        ...       ...         ...   \n",
       "632698  0.230936  0.248003  0.257737   0.249031  0.282837    0.226353   \n",
       "632699  0.167198  0.192693  0.133136   0.180714  0.142688    0.212325   \n",
       "632700  0.154964  0.171299  0.151572   0.169375  0.135997    0.167799   \n",
       "632701  0.262074  0.276528  0.274755   0.264907  0.285704    0.311325   \n",
       "632702  0.258330  0.282772  0.274016   0.253963  0.258670    0.336860   \n",
       "\n",
       "             xgb     cat_2   cat_240   cat_101    cat_16    cat_24  \n",
       "0       0.742674  0.557389  0.520828  0.510867  0.522435  0.344089  \n",
       "1       0.134947  0.160330  0.149793  0.155188  0.180702  0.171974  \n",
       "2       0.177632  0.164874  0.152439  0.156001  0.167668  0.150325  \n",
       "3       0.269195  0.267091  0.235199  0.255933  0.249617  0.219441  \n",
       "4       0.181404  0.292495  0.254902  0.287250  0.256893  0.255608  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "632698  0.274747  0.245429  0.240252  0.254258  0.257292  0.218767  \n",
       "632699  0.138745  0.168712  0.177035  0.164318  0.214233  0.141854  \n",
       "632700  0.163699  0.178346  0.162533  0.182450  0.158352  0.170247  \n",
       "632701  0.289288  0.301740  0.280562  0.294510  0.298746  0.277619  \n",
       "632702  0.286293  0.245553  0.241083  0.258185  0.227552  0.273197  \n",
       "\n",
       "[632703 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-eclipse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] C=5.362030922832332e-06 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=5.362030922832332e-06, score=-0.693, total=   0.4s\n",
      "[CV] C=5.362030922832332e-06 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=5.362030922832332e-06, score=-0.693, total=   0.4s\n",
      "[CV] C=5.362030922832332e-06 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=5.362030922832332e-06, score=-0.693, total=   0.4s\n",
      "[CV] C=1.801544954615117e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=1.801544954615117e-05, score=-0.550, total=   0.5s\n",
      "[CV] C=1.801544954615117e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=1.801544954615117e-05, score=-0.550, total=   0.5s\n",
      "[CV] C=1.801544954615117e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=1.801544954615117e-05, score=-0.550, total=   0.5s\n",
      "[CV] C=6.052863682078157e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=6.052863682078157e-05, score=-0.512, total=   0.6s\n",
      "[CV] C=6.052863682078157e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    3.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=6.052863682078157e-05, score=-0.512, total=   0.5s\n",
      "[CV] C=6.052863682078157e-05 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    3.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ C=6.052863682078157e-05, score=-0.512, total=   0.6s\n",
      "[CV] C=0.00020336522083429175 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    4.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... C=0.00020336522083429175, score=-0.462, total=   0.9s\n",
      "[CV] C=0.00020336522083429175 ........................................\n",
      "[CV] ........... C=0.00020336522083429175, score=-0.460, total=   0.9s\n",
      "[CV] C=0.00020336522083429175 ........................................\n",
      "[CV] ........... C=0.00020336522083429175, score=-0.464, total=   1.0s\n",
      "[CV] C=0.0006832701877531929 .........................................\n",
      "[CV] ............ C=0.0006832701877531929, score=-0.439, total=   1.3s\n",
      "[CV] C=0.0006832701877531929 .........................................\n",
      "[CV] ............ C=0.0006832701877531929, score=-0.431, total=   1.2s\n",
      "[CV] C=0.0006832701877531929 .........................................\n",
      "[CV] ............ C=0.0006832701877531929, score=-0.445, total=   1.2s\n",
      "[CV] C=0.0022956636712857296 .........................................\n",
      "[CV] ............ C=0.0022956636712857296, score=-0.437, total=   1.4s\n",
      "[CV] C=0.0022956636712857296 .........................................\n",
      "[CV] ............ C=0.0022956636712857296, score=-0.428, total=   1.5s\n",
      "[CV] C=0.0022956636712857296 .........................................\n",
      "[CV] ............ C=0.0022956636712857296, score=-0.442, total=   1.4s\n",
      "[CV] C=0.007713012781943162 ..........................................\n",
      "[CV] ............. C=0.007713012781943162, score=-0.437, total=   2.7s\n",
      "[CV] C=0.007713012781943162 ..........................................\n",
      "[CV] ............. C=0.007713012781943162, score=-0.428, total=   3.2s\n",
      "[CV] C=0.007713012781943162 ..........................................\n",
      "[CV] ............. C=0.007713012781943162, score=-0.442, total=   3.0s\n",
      "[CV] C=0.025914321387113172 ..........................................\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import l1_min_c\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer,log_loss\n",
    "\n",
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "cs = l1_min_c(prob_df, y_test, loss='log') * np.logspace(0, 10, 20)\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "# clf.fit(prob_df, y_test)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': list(cs)}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid, verbose=10,cv=3, scoring=LogLoss)\n",
    "grid.fit(prob_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "controversial-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.7288911003040416, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_opt = LogisticRegression(C=0.7288911003040416, penalty='l1', solver='liblinear')\n",
    "clf_opt.fit(prob_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "atlantic-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = clf_opt.coef_[0] / clf_opt.coef_[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "departmental-break",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.09637551,  0.04561797, -0.19434204,  0.03252338,\n",
       "        1.13736628, -0.01169298,  0.5342642 ,  0.4742598 , -0.64531503,\n",
       "       -0.49830387,  0.22199779])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "joint-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44779935493713907"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_proba = np.zeros((len(prob_df), ))\n",
    "final_proba+=proba_lgb_li[0] * wts[0]\n",
    "final_proba+=proba_lgb_li_8[0] * wts[1]\n",
    "final_proba+=proba_lgb_li_28[0] * wts[2]\n",
    "final_proba+=proba_lgb_li_4812[0] * wts[3]\n",
    "final_proba+=proba_lgb_li_19[0] * wts[4]\n",
    "final_proba+=proba_lgb_li_48128 * wts[5]\n",
    "final_proba+=proba_xgb_li[0] * wts[6]\n",
    "final_proba+=proba_cat_li_2[0] * wts[7]\n",
    "final_proba+=proba_cat_li_240[0] * wts[8]\n",
    "final_proba+=proba_cat_li_101[0] * wts[9]\n",
    "final_proba+=proba_cat_li_16[0] * wts[10]\n",
    "final_proba+=proba_cat_li_24[0] * wts[11]\n",
    "log_loss(y_test, final_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "resistant-walter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4487179307386303"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_proba = np.zeros((len(prob_df), ))\n",
    "final_proba+=proba_lgb_li[0] * 0.15\n",
    "final_proba+=proba_lgb_li_8[0] * 0.15\n",
    "final_proba+=proba_lgb_li_28[0] * 0.15\n",
    "final_proba+=proba_lgb_li_4812[0] * 0.15\n",
    "final_proba+=proba_lgb_li_19[0] * 0.15\n",
    "final_proba+=proba_lgb_li_48128 * 0.15\n",
    "final_proba+=proba_xgb_li[0] * 0.001\n",
    "final_proba+=proba_cat_li_2[0] * 0.001\n",
    "final_proba+=proba_cat_li_240[0] * 0.001\n",
    "final_proba+=proba_cat_li_101[0] * 0.001\n",
    "final_proba+=proba_cat_li_16[0] * 0.001\n",
    "final_proba+=proba_cat_li_24[0] * 0.095\n",
    "log_loss(y_test, final_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "solid-desperate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60432742, 0.20919254, 0.13900652, ..., 0.15464651, 0.25881689,\n",
       "       0.28517411])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "focused-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.to_csv('prob_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "norwegian-blackjack",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'repeat' is an invalid keyword argument for permutations()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6e94779c4024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'repeat' is an invalid keyword argument for permutations()"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.permutations([1,5,10],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "equipped-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 9\n",
    "\n",
    "# create placeholder for results table\n",
    "output_wts = np.zeros((len(X_test)+1, model_num+1))\n",
    "\n",
    "# getting the possible weights for three models\n",
    "import itertools\n",
    "\n",
    "j=0\n",
    "for a,b,c,d,e,f,g,h,i in itertools.product([1,5,10], repeat=model_num):\n",
    "    sum_w = np.array([a,b,c,d,e,f,g,h,i]).sum()\n",
    "    wts = np.array([a,b,c,d,e,f,g,h,i]) / sum_w\n",
    "    \n",
    "    final_proba = np.zeros((len(X_test), ))\n",
    "    #get oof combination for weighted final_probability\n",
    "    final_proba+=proba_lgb_li[0] * wts[0]\n",
    "    final_proba+=proba_lgb_li_8[0] * wts[1]\n",
    "    final_proba+=proba_lgb_li_28[0] * wts[2]\n",
    "    final_proba+=proba_xgb_li[0] * wts[3]\n",
    "    final_proba+=proba_cat_li_2[0] * wts[4]\n",
    "    final_proba+=proba_cat_li_240[0] * wts[5]\n",
    "    final_proba+=proba_cat_li_101[0] * wts[6]\n",
    "    final_proba+=proba_cat_li_16[0] * wts[7]\n",
    "    final_proba+=proba_cat_li_24[0] * wts[8]\n",
    "    final_proba+=proba_lgb_li_4812[0] * wts[9]\n",
    "\n",
    "    #get the logloss of weighted probability for i-fold\n",
    "    output_wts[j,model_num] = log_loss(y_test, final_proba)\n",
    "\n",
    "    #record the associated weights\n",
    "    output_wts[j,0:model_num] = wts\n",
    "\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bottom-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wts = pd.DataFrame(columns=['lgbm','lgbm_8','lgbm_28','xgb','cat_2','cat_240',\n",
    "                               'cat_101','cat_16','cat_24','logloss'],data=output_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "improved-target",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm</th>\n",
       "      <th>lgbm_8</th>\n",
       "      <th>lgbm_28</th>\n",
       "      <th>xgb</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_240</th>\n",
       "      <th>cat_101</th>\n",
       "      <th>cat_16</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16768</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.454346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18956</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.454303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18955</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.454274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgbm    lgbm_8   lgbm_28       xgb     cat_2   cat_240   cat_101  \\\n",
       "16768  0.285714  0.142857  0.285714  0.028571  0.028571  0.028571  0.028571   \n",
       "18956  0.222222  0.222222  0.222222  0.022222  0.022222  0.022222  0.022222   \n",
       "18955  0.250000  0.250000  0.250000  0.025000  0.025000  0.025000  0.025000   \n",
       "\n",
       "         cat_16    cat_24   logloss  \n",
       "16768  0.028571  0.142857  0.454346  \n",
       "18956  0.022222  0.222222  0.454303  \n",
       "18955  0.025000  0.125000  0.454274  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wts.sort_values('logloss', ascending=False).iloc[19680:19683,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-wrapping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
