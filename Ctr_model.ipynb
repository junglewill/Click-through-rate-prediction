{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharp-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.fromnumeric import _all_dispatcher\n",
    "import pandas as pd\n",
    "import joblib\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tight-mitchell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train.gz...\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "######### training code (without any validation) #########\n",
    "\n",
    "# load data\n",
    "print('loading train.gz...')\n",
    "# use only a subset of rows - you should use all rows eventually\n",
    "df_train = pd.read_csv(\"train.gz\", compression='gzip', nrows=20000, header='infer')\n",
    "Y = df_train['click']\n",
    "# discard some columns\n",
    "unused_cols = [\"id\", 'site_id', 'app_id']\n",
    "df_train.drop(unused_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modern-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy to prevent from modifying the original dataset\n",
    "df_copy = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reasonable-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique() shows that the three columns are same across \n",
    "df_copy.drop(['hour', 'app_domain', 'app_category', 'click'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "white-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_copy, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "white-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14000 entries, 10657 to 6201\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   C1                14000 non-null  int64 \n",
      " 1   banner_pos        14000 non-null  int64 \n",
      " 2   site_domain       14000 non-null  object\n",
      " 3   site_category     14000 non-null  object\n",
      " 4   device_id         14000 non-null  object\n",
      " 5   device_ip         14000 non-null  object\n",
      " 6   device_model      14000 non-null  object\n",
      " 7   device_type       14000 non-null  int64 \n",
      " 8   device_conn_type  14000 non-null  int64 \n",
      " 9   C14               14000 non-null  int64 \n",
      " 10  C15               14000 non-null  int64 \n",
      " 11  C16               14000 non-null  int64 \n",
      " 12  C17               14000 non-null  int64 \n",
      " 13  C18               14000 non-null  int64 \n",
      " 14  C19               14000 non-null  int64 \n",
      " 15  C20               14000 non-null  int64 \n",
      " 16  C21               14000 non-null  int64 \n",
      "dtypes: int64(12), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "further-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# one_hot_features = ['C1', 'device_type', 'device_conn_type', 'C18']\n",
    "one_hot_features = [0,7,8,13]\n",
    "one_hot_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# target_features = ['banner_pos','site_domain','site_category','device_id','device_ip','device_model','C14',\n",
    "#                    'C15','C16','C17','C19','C20','C21']\n",
    "target_features = [1,2,3,4,5,6,9,10,11,12,14,15,16]\n",
    "target_transformer = ce.JamesSteinEncoder()\n",
    "        \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot', one_hot_transformer, one_hot_features),\n",
    "        ('target', target_transformer, target_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mineral-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6160399\ttotal: 4.02ms\tremaining: 76.4ms\n",
      "1:\tlearn: 0.5533086\ttotal: 7.28ms\tremaining: 65.5ms\n",
      "2:\tlearn: 0.5003023\ttotal: 10.1ms\tremaining: 57ms\n",
      "3:\tlearn: 0.4554998\ttotal: 13.2ms\tremaining: 52.7ms\n",
      "4:\tlearn: 0.4170045\ttotal: 16.4ms\tremaining: 49.1ms\n",
      "5:\tlearn: 0.3836853\ttotal: 18.5ms\tremaining: 43.1ms\n",
      "6:\tlearn: 0.3553075\ttotal: 21ms\tremaining: 38.9ms\n",
      "7:\tlearn: 0.3301982\ttotal: 23.8ms\tremaining: 35.7ms\n",
      "8:\tlearn: 0.3088449\ttotal: 26.6ms\tremaining: 32.5ms\n",
      "9:\tlearn: 0.2898885\ttotal: 30ms\tremaining: 30ms\n",
      "10:\tlearn: 0.2740571\ttotal: 33.1ms\tremaining: 27.1ms\n",
      "11:\tlearn: 0.2594245\ttotal: 35.9ms\tremaining: 23.9ms\n",
      "12:\tlearn: 0.2470110\ttotal: 39.4ms\tremaining: 21.2ms\n",
      "13:\tlearn: 0.2355591\ttotal: 42.9ms\tremaining: 18.4ms\n",
      "14:\tlearn: 0.2252611\ttotal: 47ms\tremaining: 15.7ms\n",
      "15:\tlearn: 0.2156349\ttotal: 49.6ms\tremaining: 12.4ms\n",
      "16:\tlearn: 0.2068159\ttotal: 52.3ms\tremaining: 9.23ms\n",
      "17:\tlearn: 0.2003635\ttotal: 56.2ms\tremaining: 6.24ms\n",
      "18:\tlearn: 0.1942897\ttotal: 58.7ms\tremaining: 3.09ms\n",
      "19:\tlearn: 0.1878198\ttotal: 62.7ms\tremaining: 0us\n",
      "model logloss: 0.498\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', CatBoostClassifier(iterations=20,learning_rate=0.1,depth=7, eval_metric='Logloss'))])\n",
    "\n",
    "clf.fit(X_train.values, y_train.values.reshape(-1,1))\n",
    "y_pred = clf.predict_proba(X_test.values)[:, 1]\n",
    "print(\"model logloss: %.3f\" % log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-decline",
   "metadata": {},
   "source": [
    "#### First baseline model has a very high logloss at 0.525."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "confidential-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6538872\ttotal: 14.2ms\tremaining: 270ms\n",
      "1:\tlearn: 0.6239209\ttotal: 22.4ms\tremaining: 201ms\n",
      "2:\tlearn: 0.5968163\ttotal: 29.5ms\tremaining: 167ms\n",
      "3:\tlearn: 0.5743491\ttotal: 37.6ms\tremaining: 150ms\n",
      "4:\tlearn: 0.5560601\ttotal: 42.7ms\tremaining: 128ms\n",
      "5:\tlearn: 0.5410683\ttotal: 47.1ms\tremaining: 110ms\n",
      "6:\tlearn: 0.5274857\ttotal: 55.6ms\tremaining: 103ms\n",
      "7:\tlearn: 0.5156095\ttotal: 62.2ms\tremaining: 93.2ms\n",
      "8:\tlearn: 0.5057243\ttotal: 66.1ms\tremaining: 80.8ms\n",
      "9:\tlearn: 0.4972880\ttotal: 74ms\tremaining: 74ms\n",
      "10:\tlearn: 0.4904956\ttotal: 78.2ms\tremaining: 64ms\n",
      "11:\tlearn: 0.4842027\ttotal: 85.4ms\tremaining: 56.9ms\n",
      "12:\tlearn: 0.4787759\ttotal: 93.5ms\tremaining: 50.3ms\n",
      "13:\tlearn: 0.4747286\ttotal: 97.8ms\tremaining: 41.9ms\n",
      "14:\tlearn: 0.4708115\ttotal: 105ms\tremaining: 34.9ms\n",
      "15:\tlearn: 0.4677292\ttotal: 111ms\tremaining: 27.9ms\n",
      "16:\tlearn: 0.4646293\ttotal: 118ms\tremaining: 20.8ms\n",
      "17:\tlearn: 0.4629520\ttotal: 121ms\tremaining: 13.5ms\n",
      "18:\tlearn: 0.4606878\ttotal: 127ms\tremaining: 6.7ms\n",
      "19:\tlearn: 0.4586883\ttotal: 134ms\tremaining: 0us\n",
      "model logloss: 0.463\n"
     ]
    }
   ],
   "source": [
    "categorical_f = ['C1','device_type','device_conn_type','C18','banner_pos','site_domain','site_category',\n",
    "                 'device_id','device_ip','device_model','C14','C15','C16','C17','C19','C20','C21']\n",
    "cat = CatBoostClassifier(iterations=20,learning_rate=0.1,depth=7, eval_metric='Logloss')\n",
    "cat.fit(X_train, y_train,cat_features=categorical_f)\n",
    "y_pred_cat = cat.predict_proba(X_test.values)[:, 1]\n",
    "print(\"model logloss: %.3f\" % log_loss(y_test, y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-apache",
   "metadata": {},
   "source": [
    "#### using the default catboost encoder seems to get much better result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-literacy",
   "metadata": {},
   "source": [
    "### Bayesian Search CV for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "satellite-breast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5, error_score='raise',\n",
       "              estimator=<catboost.core.CatBoostClassifier object at 0x7ff8930bd3d0>,\n",
       "              fit_params=None, iid=True, n_iter=64, n_jobs=1, n_points=1,\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=42,\n",
       "              refit=True, return_train_score=False,\n",
       "              scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       "              search_spaces={'bagg...\n",
       "                             'iterations': Integer(low=10, high=1000, prior='uniform', transform='identity'),\n",
       "                             'l2_leaf_reg': Integer(low=2, high=30, prior='uniform', transform='identity'),\n",
       "                             'learning_rate': Real(low=0.01, high=1.0, prior='log-uniform', transform='identity'),\n",
       "                             'random_strength': Real(low=1e-09, high=10, prior='log-uniform', transform='identity'),\n",
       "                             'scale_pos_weight': Real(low=0.01, high=1.0, prior='uniform', transform='identity')},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "cat1 = CatBoostClassifier(iterations=20,learning_rate=0.1,depth=7,loss_function='Logloss', \n",
    "                          cat_features=categorical_f,verbose=False)\n",
    "\n",
    "param = {\n",
    "    'iterations': Integer(10, 1000),\n",
    "    'depth': Integer(1, 8),\n",
    "    'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "    'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
    "    'bagging_temperature': Real(0.0, 1.0),\n",
    "    'border_count': Integer(1, 255),\n",
    "    'l2_leaf_reg': Integer(2, 30),\n",
    "    'scale_pos_weight':Real(0.01, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    cat1,\n",
    "    param,\n",
    "    scoring = LogLoss,\n",
    "    n_iter=64,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "understanding-moses",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bagging_temperature', 0.0),\n",
       "             ('border_count', 48),\n",
       "             ('depth', 8),\n",
       "             ('iterations', 759),\n",
       "             ('l2_leaf_reg', 3),\n",
       "             ('learning_rate', 0.01),\n",
       "             ('random_strength', 1e-09),\n",
       "             ('scale_pos_weight', 1.0)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "educated-counter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.43869105458047825"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params={'bagging_temperature': 0.0,\n",
    " 'border_count': 48,\n",
    " 'depth': 8,\n",
    " 'iterations': 759,\n",
    " 'l2_leaf_reg': 3,\n",
    " 'learning_rate': 0.01,\n",
    " 'random_strength': 1e-09,\n",
    " 'scale_pos_weight': 1.0}\n",
    "\n",
    "best_params['iterations'] = 1000\n",
    "\n",
    "from imb\n",
    "\n",
    "opt_cat = CatBoostClassifier(**best_params, task_type = \"GPU\",od_type='Iter', loss_function='Logloss')\n",
    "opt_cat.fit(X_train, y_train,cat_features=categorical_f)\n",
    "y_pred_cat = cat.predict_proba(X_test.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "analyzed-classification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11312\n",
       "1     2688\n",
       "Name: click, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "broad-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14000 entries, 10657 to 6201\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   C1                14000 non-null  int64 \n",
      " 1   banner_pos        14000 non-null  int64 \n",
      " 2   site_domain       14000 non-null  object\n",
      " 3   site_category     14000 non-null  object\n",
      " 4   device_id         14000 non-null  object\n",
      " 5   device_ip         14000 non-null  object\n",
      " 6   device_model      14000 non-null  object\n",
      " 7   device_type       14000 non-null  int64 \n",
      " 8   device_conn_type  14000 non-null  int64 \n",
      " 9   C14               14000 non-null  int64 \n",
      " 10  C15               14000 non-null  int64 \n",
      " 11  C16               14000 non-null  int64 \n",
      " 12  C17               14000 non-null  int64 \n",
      " 13  C18               14000 non-null  int64 \n",
      " 14  C19               14000 non-null  int64 \n",
      " 15  C20               14000 non-null  int64 \n",
      " 16  C21               14000 non-null  int64 \n",
      "dtypes: int64(12), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# need to balance the data\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization. \n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(random_params, i)\n",
    "        results.loc[i, :] = eval_results\n",
    "\n",
    "        # open connection (append option) and write results\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        # make sure to close connection\n",
    "        of_connection.close()\n",
    "        \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-cleanup",
   "metadata": {},
   "source": [
    "### Logistic Regression (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "handy-party",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding...\n",
      "imported data & built a vectorizer on the training set\n",
      "n = 20000, d = 14076\n"
     ]
    }
   ],
   "source": [
    "# embedding (all features are categorical)\n",
    "print('embedding...')\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('X_train_dict.pkl', 'rb') as ff:\n",
    "        X_train_dict = pickle.load(ff)\n",
    "    vectorizer = joblib.load('vectorizer.joblib')\n",
    "    X_train = vectorizer.transform(X_train_dict)\n",
    "    print('saved vectorizer loaded & applied to training set')\n",
    "except:\n",
    "    X_train_dict = list(df_copy.drop('click', axis=1).T.to_dict().values())\n",
    "    with open('X_train_dict.pkl', 'wb') as ff:\n",
    "        pickle.dump(X_train_dict, ff)\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X_train = vectorizer.fit_transform(X_train_dict) # can only see training dataset\n",
    "    joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "    print('imported data & built a vectorizer on the training set')\n",
    "\n",
    "n, d = X_train.shape\n",
    "print(\"n = {}, d = {}\".format(n, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heard-dining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x14076 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 340000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wrapped-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit a simple logistic regression with l1 regularization...\n",
      "...done training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "print('fit a simple logistic regression with l1 regularization...')\n",
    "clf = LogisticRegression(max_iter=20000, penalty='l1', solver='liblinear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print('...done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "geographic-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and transforming test data...\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "######### testing code ###################################\n",
    "\n",
    "# transform test data as well\n",
    "print('loading and transforming test data...')\n",
    "df_test = pd.read_csv(\"test.gz\", compression='gzip', header='infer')\n",
    "# df_test.set_index('id', inplace=True)\n",
    "unused_cols = ['site_id', 'app_id']\n",
    "df_test.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "try:\n",
    "    with open('X_test_dict.pkl', 'rb') as ff:\n",
    "        X_test_dict = pickle.load('ff')\n",
    "except:\n",
    "    X_test_dict = list(df_test.T.to_dict().values())\n",
    "    with open('X_test_dict.pkl', 'wb') as ff:\n",
    "        pickle.dump(X_test_dict, ff)\n",
    "\n",
    "X_test = vectorizer.transform(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "champion-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting and output to csv...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "print('predicting and output to csv...')\n",
    "ctr_pred = clf.predict_proba(X_test)[:, 1]\n",
    "# save output: every line is (id, ctr_pred)\n",
    "all_id = df_test['id']\n",
    "df_out = pd.DataFrame({'id': all_id, 'ctr': ctr_pred})\n",
    "df_out.to_csv('Submission.csv', index=False)\n",
    "\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-physics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
